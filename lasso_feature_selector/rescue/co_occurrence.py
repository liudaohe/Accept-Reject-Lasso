"""
Co-occurrence based feature rescue.

This module implements the co-occurrence rescue algorithm that identifies
features to rescue based on their joint selection patterns across subsets.
"""

import numpy as np
import pandas as pd
from collections import Counter
from itertools import combinations
from typing import Set, List, Dict, Any
from tqdm.auto import tqdm

from ..core.base import BaseRescueMethod
from ..core.types import DataFrame, Series, FeatureSet, Metadata, ProblemGroups, CoOccurrenceCounter
from ..config.defaults import RESCUE_DEFAULTS

class CoOccurrenceRescue(BaseRescueMethod):
    """
    Co-occurrence based feature rescue method.
    
    This method rescues features based on their co-occurrence patterns
    in subset selections, identifying features that are consistently
    selected together across different data subsets.
    """
    
    def __init__(self, **kwargs):
        """
        Initialize co-occurrence rescue method.
        
        Args:
            **kwargs: Configuration overrides
        """
        config = {**RESCUE_DEFAULTS, **kwargs}
        super().__init__(**config)
        
        self.co_occurrence_threshold = config['co_occurrence_threshold']
        self.n_final_clusters = config['n_final_clusters']
        self.min_subset_size = config['min_subset_size']
    
    @property
    def name(self) -> str:
        """Return the method name."""
        return "Co-occurrence Rescue"
    
    def identify_problem_groups(self, X: DataFrame, **kwargs) -> ProblemGroups:
        """
        Use the base class implementation for problem group identification.
        
        Args:
            X: Feature matrix
            **kwargs: Additional parameters
            
        Returns:
            List of problem groups
        """
        return self._find_connected_components(self._build_correlation_graph(X))
    
    def rescue_features_by_co_occurrence(self,
                                       problem_groups: ProblemGroups,
                                       subset_selections: List[FeatureSet],
                                       threshold: int = None) -> FeatureSet:
        """
        Execute co-occurrence rescue algorithm with unified Apriori-based pruning.

        Args:
            problem_groups: List of problem groups
            subset_selections: Feature selections from different subsets
            threshold: Co-occurrence threshold (default: use instance threshold)

        Returns:
            Set of rescued features
        """
        if threshold is None:
            threshold = self.co_occurrence_threshold

        if self.parameters.get('verbose', True):
            print(f"\\nğŸ” ç®—æ³•æ´å¯Ÿ (æ­¥éª¤5): æ­£åœ¨æ ¹æ®å­é›†åˆ†æç»“æœæ‹¯æ•‘ç‰¹å¾ (å…±ç°é˜ˆå€¼: > {threshold})...")

        rescued_features = set()

        # Use unified Apriori-based algorithm if enabled
        if self.parameters.get('use_unified_apriori', True):
            rescued_features = self._unified_apriori_rescue(
                problem_groups, subset_selections, threshold
            )
        else:
            # Fallback to original implementation
            rescued_features = self._original_co_occurrence_rescue(
                problem_groups, subset_selections, threshold
            )

        if self.parameters.get('verbose', True):
            print(f"  âœ… æˆåŠŸæ‹¯æ•‘äº† {len(rescued_features)} ä¸ªæ–°ç‰¹å¾ã€‚")

        return rescued_features
    
    def rescue_features(self, 
                       X: DataFrame,
                       y: Series,
                       base_features: FeatureSet,
                       problem_groups: ProblemGroups,
                       **kwargs) -> FeatureSet:
        """
        Main rescue method that orchestrates the entire rescue process.
        
        Args:
            X: Feature matrix
            y: Target vector
            base_features: Initially selected features
            problem_groups: Identified problem groups
            **kwargs: Additional parameters including subset_lasso_provider and metadata
            
        Returns:
            Set of rescued features
        """
        # Extract required parameters
        subset_lasso_provider = kwargs.get('subset_lasso_provider')
        metadata = kwargs.get('metadata', {})
        clustering_basis_groups = kwargs.get('clustering_basis_groups', [])
        
        if subset_lasso_provider is None:
            raise ValueError("subset_lasso_provider is required for feature rescue")
        
        if not problem_groups:
            return set()
        
        # Use clustering basis groups if provided, otherwise use all problem groups
        if not clustering_basis_groups:
            if self.parameters.get('verbose', True):
                print("\\n  âš ï¸ è­¦å‘Š: å¤–éƒ¨æœªæä¾›èšç±»ä¾æ®ç»„ï¼Œä½¿ç”¨æ‰€æœ‰é—®é¢˜ç»„ä½œä¸ºåå¤‡æ–¹æ¡ˆã€‚")
            clustering_basis_groups = problem_groups
        
        # Import clustering analyzer for data partitioning
        from .clustering import ClusteringAnalyzer
        clustering_analyzer = ClusteringAnalyzer(
            n_final_clusters=self.n_final_clusters,
            random_state=self.random_state,
            verbose=self.parameters.get('verbose', True)
        )
        
        # Partition data based on clustering basis groups
        final_clusters = clustering_analyzer.partition_data_by_basis_groups(
            X, clustering_basis_groups, self.n_final_clusters
        )
        
        if final_clusters.size == 0:
            return set()
        
        # Run Lasso on subsets
        if self.parameters.get('verbose', True):
            print(f"\\nğŸ” ç®—æ³•æ´å¯Ÿ (æ­¥éª¤4): æ­£åœ¨å¯¹ {self.n_final_clusters} ä¸ªå­é›†è¿è¡ŒLassoåˆ†æ...")
        
        # Prepare features for subset analysis
        features_for_subset_analysis = sorted(
            list(set(feat for group in problem_groups for feat in group) | base_features)
        )
        
        subset_selections = []
        for i in tqdm(range(self.n_final_clusters), desc="  åˆ†ææ•°æ®å­é›†", 
                     disable=not self.parameters.get('verbose', True)):
            subset_indices = np.where(final_clusters == i)[0]
            
            if len(subset_indices) < self.min_subset_size:
                continue
            
            X_subset = X.iloc[subset_indices][features_for_subset_analysis]
            y_subset = y.iloc[subset_indices]
            
            # Run subset Lasso provider
            selected_set = subset_lasso_provider(X_subset, y_subset, metadata)
            
            if selected_set:
                subset_selections.append(selected_set)
        
        if self.parameters.get('verbose', True):
            print(f"  âœ… åœ¨ {len(subset_selections)} ä¸ªå­é›†ä¸Šå®Œæˆäº†Lassoåˆ†æã€‚")
        
        # Execute co-occurrence rescue
        rescued_features = self.rescue_features_by_co_occurrence(
            problem_groups, subset_selections, self.co_occurrence_threshold
        )
        
        return rescued_features

    def _unified_apriori_rescue(self,
                               problem_groups: ProblemGroups,
                               subset_selections: List[FeatureSet],
                               threshold: int) -> FeatureSet:
        """
        ç»Ÿä¸€çš„Aprioriç®—æ³•å®ç°ï¼Œä»1-é¡¹é›†å¼€å§‹é€å±‚å‰ªæåˆ°k-é¡¹é›†ã€‚

        æ ¸å¿ƒæ€æƒ³ï¼š
        1. ä»1-é¡¹é›†å¼€å§‹ï¼Œç»Ÿè®¡æ¯ä¸ªç‰¹å¾çš„å‡ºç°é¢‘ç‡
        2. åªæœ‰é¢‘ç¹çš„1-é¡¹é›†æ‰èƒ½ç»„æˆå€™é€‰2-é¡¹é›†
        3. åªæœ‰é¢‘ç¹çš„2-é¡¹é›†æ‰èƒ½ç»„æˆå€™é€‰3-é¡¹é›†
        4. ä»¥æ­¤ç±»æ¨ï¼Œå¤§å¹…å‡å°‘éœ€è¦æ£€æŸ¥çš„ç»„åˆæ•°é‡

        AprioriåŸç†ï¼šå¦‚æœä¸€ä¸ªk-é¡¹é›†ä¸é¢‘ç¹ï¼Œé‚£ä¹ˆåŒ…å«å®ƒçš„æ‰€æœ‰(k+1)-é¡¹é›†éƒ½ä¸é¢‘ç¹

        Args:
            problem_groups: é—®é¢˜ç»„åˆ—è¡¨
            subset_selections: å­é›†é€‰æ‹©ç»“æœ
            threshold: å…±ç°é˜ˆå€¼

        Returns:
            æ‹¯æ•‘çš„ç‰¹å¾é›†åˆ
        """
        if self.parameters.get('verbose', True):
            print(f"  ğŸš€ ä½¿ç”¨ç»Ÿä¸€Aprioriç®—æ³•è¿›è¡Œé¢‘ç¹é¡¹é›†æŒ–æ˜...")

        rescued_features = set()
        total_combinations_checked = 0
        total_combinations_pruned = 0

        for group_idx, group in enumerate(problem_groups):
            if len(group) < 2:
                continue

            # ä¸ºå½“å‰ç»„æ„å»ºäº‹åŠ¡æ•°æ®åº“ï¼ˆæ¯ä¸ªå­é›†é€‰æ‹©æ˜¯ä¸€ä¸ªäº‹åŠ¡ï¼‰
            transactions = []
            for selection in subset_selections:
                intersection = set(group).intersection(selection)
                if intersection:
                    transactions.append(intersection)

            if not transactions:
                continue

            # æ‰§è¡ŒAprioriç®—æ³•ï¼Œä»1-é¡¹é›†å¼€å§‹é€å±‚å‰ªæ
            frequent_itemsets, stats = self._apriori_frequent_mining(
                transactions, threshold, sorted(list(group))
            )

            # ç»Ÿè®¡ä¿¡æ¯
            checked, pruned = stats
            total_combinations_checked += checked
            total_combinations_pruned += pruned

            # æ”¶é›†æ‰€æœ‰é¢‘ç¹é¡¹é›†ä¸­çš„ç‰¹å¾ï¼ˆåªè€ƒè™‘2-é¡¹é›†åŠä»¥ä¸Šï¼‰
            for size, itemsets in frequent_itemsets.items():
                if size >= 2:
                    for itemset in itemsets:
                        rescued_features.update(itemset)

        if self.parameters.get('verbose', True):
            print(f"  ğŸ“Š Aprioriç»Ÿè®¡: æ£€æŸ¥äº† {total_combinations_checked} ä¸ªç»„åˆï¼Œå‰ªæäº† {total_combinations_pruned} ä¸ªç»„åˆ")
            if total_combinations_checked + total_combinations_pruned > 0:
                pruning_ratio = total_combinations_pruned / (total_combinations_checked + total_combinations_pruned)
                print(f"  ğŸ“Š å‰ªææ•ˆç‡: {pruning_ratio:.2%}")

        return rescued_features

    def _apriori_frequent_mining(self, transactions: List[Set[str]],
                                min_support: int,
                                all_items: List[str]) -> tuple:
        """
        æ”¹è¿›çš„Apriorié¢‘ç¹é¡¹é›†æŒ–æ˜ç®—æ³•

        ä»1-é¡¹é›†å¼€å§‹ï¼Œé€å±‚å‘ä¸ŠæŒ–æ˜é¢‘ç¹é¡¹é›†ï¼Œå……åˆ†åˆ©ç”¨AprioriåŸç†è¿›è¡Œå‰ªæï¼š
        å¦‚æœä¸€ä¸ªk-é¡¹é›†ä¸é¢‘ç¹ï¼Œé‚£ä¹ˆåŒ…å«å®ƒçš„æ‰€æœ‰(k+1)-é¡¹é›†éƒ½ä¸é¢‘ç¹

        Args:
            transactions: äº‹åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹åŠ¡æ˜¯ä¸€ä¸ªç‰¹å¾é›†åˆ
            min_support: æœ€å°æ”¯æŒåº¦ï¼ˆç»å¯¹é¢‘æ¬¡ï¼‰
            all_items: æ‰€æœ‰å¯èƒ½çš„é¡¹ç›®åˆ—è¡¨

        Returns:
            (frequent_itemsets_dict, (combinations_checked, combinations_pruned))
        """
        frequent_itemsets = {}
        combinations_checked = 0
        combinations_pruned = 0

        # ç¬¬1å±‚ï¼šç»Ÿè®¡1-é¡¹é›†é¢‘ç‡ï¼ˆè¿™æ˜¯æ‰€æœ‰åç»­å‰ªæçš„åŸºç¡€ï¼‰
        item_counts = Counter()
        for transaction in transactions:
            for item in transaction:
                if item in all_items:
                    item_counts[item] += 1

        # æ‰¾å‡ºé¢‘ç¹1-é¡¹é›†
        frequent_1_itemsets = []
        for item, count in item_counts.items():
            combinations_checked += 1
            if count > min_support:
                frequent_1_itemsets.append(frozenset([item]))
            else:
                combinations_pruned += 1  # ä¸é¢‘ç¹çš„1-é¡¹é›†è¢«å‰ªæ

        if not frequent_1_itemsets:
            return {}, (combinations_checked, combinations_pruned)

        frequent_itemsets[1] = frequent_1_itemsets

        # ä»2-é¡¹é›†å¼€å§‹è¿­ä»£ï¼Œæ¯ä¸€å±‚éƒ½åŸºäºä¸Šä¸€å±‚çš„é¢‘ç¹é¡¹é›†
        k = 2
        current_frequent = frequent_1_itemsets

        while current_frequent and k <= len(all_items):
            # ç”Ÿæˆå€™é€‰k-é¡¹é›†ï¼ˆåªä»é¢‘ç¹(k-1)-é¡¹é›†ç”Ÿæˆï¼‰
            candidates = self._generate_candidates_apriori(current_frequent, k)

            if not candidates:
                break

            # è®¡ç®—å€™é€‰é¡¹é›†çš„æ”¯æŒåº¦
            candidate_counts = Counter()
            for transaction in transactions:
                for candidate in candidates:
                    if candidate.issubset(transaction):
                        candidate_counts[candidate] += 1

            # æ‰¾å‡ºé¢‘ç¹k-é¡¹é›†
            frequent_k_itemsets = []
            for candidate in candidates:
                combinations_checked += 1
                count = candidate_counts.get(candidate, 0)
                if count > min_support:
                    frequent_k_itemsets.append(candidate)
                else:
                    combinations_pruned += 1  # ä¸é¢‘ç¹çš„k-é¡¹é›†è¢«å‰ªæ

            if frequent_k_itemsets:
                frequent_itemsets[k] = frequent_k_itemsets
                current_frequent = frequent_k_itemsets
            else:
                current_frequent = []

            k += 1

        return frequent_itemsets, (combinations_checked, combinations_pruned)

    def _generate_candidates_apriori(self, frequent_k_minus_1: List[frozenset], k: int) -> List[frozenset]:
        """
        åŸºäºAprioriåŸç†ä»(k-1)-é¢‘ç¹é¡¹é›†ç”Ÿæˆk-å€™é€‰é¡¹é›†

        æ ¸å¿ƒæ€æƒ³ï¼š
        1. è¿æ¥æ­¥éª¤ï¼šä¸¤ä¸ª(k-1)-é¡¹é›†å¦‚æœå‰(k-2)ä¸ªå…ƒç´ ç›¸åŒï¼Œåˆ™å¯ä»¥è¿æ¥ç”Ÿæˆk-é¡¹é›†
        2. å‰ªææ­¥éª¤ï¼šå¦‚æœå€™é€‰k-é¡¹é›†çš„ä»»ä½•(k-1)-å­é›†ä¸é¢‘ç¹ï¼Œåˆ™å‰ªæè¯¥å€™é€‰

        Args:
            frequent_k_minus_1: (k-1)-é¢‘ç¹é¡¹é›†åˆ—è¡¨
            k: å½“å‰é¡¹é›†å¤§å°

        Returns:
            k-å€™é€‰é¡¹é›†åˆ—è¡¨
        """
        candidates = []
        n = len(frequent_k_minus_1)

        # è¿æ¥æ­¥éª¤ï¼šä»é¢‘ç¹(k-1)-é¡¹é›†ç”Ÿæˆå€™é€‰k-é¡¹é›†
        for i in range(n):
            for j in range(i + 1, n):
                itemset1 = sorted(list(frequent_k_minus_1[i]))
                itemset2 = sorted(list(frequent_k_minus_1[j]))

                # åªæœ‰å½“ä¸¤ä¸ª(k-1)-é¡¹é›†çš„å‰(k-2)ä¸ªå…ƒç´ ç›¸åŒæ—¶æ‰èƒ½è¿æ¥
                if k == 2:
                    # å¯¹äº2-é¡¹é›†ï¼Œä»»æ„ä¸¤ä¸ª1-é¡¹é›†éƒ½å¯ä»¥è¿æ¥
                    candidate = frozenset(itemset1 + itemset2)
                    candidates.append(candidate)
                else:
                    # å¯¹äºk>=3çš„é¡¹é›†ï¼Œéœ€è¦å‰(k-2)ä¸ªå…ƒç´ ç›¸åŒ
                    if itemset1[:-1] == itemset2[:-1]:
                        candidate = frozenset(itemset1 + [itemset2[-1]])

                        # å‰ªææ­¥éª¤ï¼šæ£€æŸ¥å€™é€‰é¡¹é›†çš„æ‰€æœ‰(k-1)-å­é›†æ˜¯å¦éƒ½æ˜¯é¢‘ç¹çš„
                        if not self._has_infrequent_subset_apriori(candidate, frequent_k_minus_1, k-1):
                            candidates.append(candidate)
                        # å¦‚æœæœ‰ä¸é¢‘ç¹çš„å­é›†ï¼Œåˆ™è¯¥å€™é€‰è¢«å‰ªæï¼ˆä¸æ·»åŠ åˆ°candidatesä¸­ï¼‰

        return candidates

    def _has_infrequent_subset_apriori(self, candidate: frozenset,
                                      frequent_k_minus_1: List[frozenset],
                                      k_minus_1: int) -> bool:
        """
        åŸºäºAprioriåŸç†æ£€æŸ¥å€™é€‰é¡¹é›†æ˜¯å¦åŒ…å«éé¢‘ç¹çš„(k-1)-å­é›†

        AprioriåŸç†ï¼šå¦‚æœä¸€ä¸ªé¡¹é›†ä¸é¢‘ç¹ï¼Œé‚£ä¹ˆåŒ…å«å®ƒçš„æ‰€æœ‰è¶…é›†éƒ½ä¸é¢‘ç¹
        åè¿‡æ¥è¯´ï¼šå¦‚æœä¸€ä¸ªé¡¹é›†è¦é¢‘ç¹ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰å­é›†éƒ½å¿…é¡»é¢‘ç¹

        Args:
            candidate: å€™é€‰k-é¡¹é›†
            frequent_k_minus_1: é¢‘ç¹(k-1)-é¡¹é›†åˆ—è¡¨
            k_minus_1: å­é›†å¤§å°

        Returns:
            å¦‚æœåŒ…å«éé¢‘ç¹å­é›†åˆ™è¿”å›Trueï¼ˆéœ€è¦å‰ªæï¼‰
        """
        candidate_list = list(candidate)
        frequent_set = set(frequent_k_minus_1)

        # æ£€æŸ¥å€™é€‰é¡¹é›†çš„æ‰€æœ‰(k-1)-å­é›†æ˜¯å¦éƒ½åœ¨é¢‘ç¹(k-1)-é¡¹é›†ä¸­
        for subset in combinations(candidate_list, k_minus_1):
            if frozenset(subset) not in frequent_set:
                # å‘ç°éé¢‘ç¹å­é›†ï¼Œæ ¹æ®AprioriåŸç†ï¼Œè¯¥å€™é€‰å¿…é¡»å‰ªæ
                return True

        # æ‰€æœ‰å­é›†éƒ½é¢‘ç¹ï¼Œè¯¥å€™é€‰å¯ä»¥ä¿ç•™
        return False

    def _original_co_occurrence_rescue(self,
                                     problem_groups: ProblemGroups,
                                     subset_selections: List[FeatureSet],
                                     threshold: int) -> FeatureSet:
        """
        åŸå§‹çš„å…±ç°æ‹¯æ•‘ç®—æ³•å®ç°ï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
        """
        # ç‰¹å¾é¢„ç­›é€‰
        feature_counts = Counter()
        for selection in subset_selections:
            for feature in selection:
                feature_counts[feature] += 1

        min_appearances = max(2, threshold + 1)
        frequent_features = {feat for feat, count in feature_counts.items() if count >= min_appearances}

        if self.parameters.get('verbose', True):
            total_unique_features = len(feature_counts)
            print(f"  ğŸ“Š ä¼˜åŒ–ç»Ÿè®¡: {total_unique_features} ä¸ªå”¯ä¸€ç‰¹å¾ä¸­ï¼Œ{len(frequent_features)} ä¸ªå‡ºç°â‰¥{min_appearances}æ¬¡")

        rescued_features = set()
        co_occurrence_counter = Counter()

        # åˆ†ææ¯ä¸ªé—®é¢˜ç»„å†…çš„å…±ç°æ¨¡å¼
        for group in problem_groups:
            group_frequent = set(group).intersection(frequent_features)

            if len(group_frequent) < 2:
                continue

            # æ£€æŸ¥ä¸æ¯ä¸ªå­é›†é€‰æ‹©çš„äº¤é›†
            for selection in subset_selections:
                intersection = group_frequent.intersection(selection)

                if len(intersection) < 2:
                    continue

                # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å¤§å°â‰¥2çš„å­é›†
                for k in range(2, len(intersection) + 1):
                    for subset in combinations(sorted(list(intersection)), k):
                        co_occurrence_counter[frozenset(subset)] += 1

        # æ‹¯æ•‘è¶…è¿‡é˜ˆå€¼çš„ç‰¹å¾
        for feature_set, count in co_occurrence_counter.items():
            if count > threshold:
                rescued_features.update(feature_set)

        return rescued_features

    def _co_occurrence_with_pruning(self,
                                   problem_groups: ProblemGroups,
                                   subset_selections: List[FeatureSet],
                                   frequent_features: FeatureSet,
                                   threshold: int) -> FeatureSet:
        """
        Co-occurrence analysis with hierarchical pruning optimization.

        This method uses a bottom-up approach:
        1. First check all 2-feature combinations
        2. Only check larger combinations that contain at least one valid 2-feature pair

        Args:
            problem_groups: List of problem groups
            subset_selections: Feature selections from different subsets
            frequent_features: Pre-filtered frequent features
            threshold: Co-occurrence threshold

        Returns:
            Set of rescued features
        """
        rescued_features = set()

        if self.parameters.get('verbose', True):
            print(f"  ğŸš€ ä½¿ç”¨åˆ†å±‚å‰ªæä¼˜åŒ–è¿›è¡Œå…±ç°åˆ†æ...")

        total_combinations_checked = 0
        total_combinations_pruned = 0

        for group_idx, group in enumerate(problem_groups):
            # Only consider frequent features within this group
            group_frequent = set(group).intersection(frequent_features)

            if len(group_frequent) < 2:
                continue

            group_frequent_list = sorted(list(group_frequent))

            # Step 1: Check all 2-feature combinations and identify valid pairs
            valid_pairs = set()
            pair_counts = {}

            for pair in combinations(group_frequent_list, 2):
                count = 0
                for selection in subset_selections:
                    if set(pair).issubset(selection):
                        count += 1

                total_combinations_checked += 1
                pair_counts[frozenset(pair)] = count

                if count > threshold:
                    rescued_features.update(pair)
                    valid_pairs.add(frozenset(pair))

            # Step 2: For larger combinations, only check those containing valid pairs
            for k in range(3, len(group_frequent_list) + 1):
                for subset in combinations(group_frequent_list, k):
                    subset_set = set(subset)

                    # Pruning: Check if this subset contains at least one valid 2-feature pair
                    contains_valid_pair = False
                    for pair in combinations(subset, 2):
                        if frozenset(pair) in valid_pairs:
                            contains_valid_pair = True
                            break

                    if not contains_valid_pair:
                        total_combinations_pruned += 1
                        continue  # Prune this combination

                    # Count co-occurrences for this combination
                    count = 0
                    for selection in subset_selections:
                        if subset_set.issubset(selection):
                            count += 1

                    total_combinations_checked += 1

                    if count > threshold:
                        rescued_features.update(subset)

        if self.parameters.get('verbose', True):
            print(f"  ğŸ“Š å‰ªæç»Ÿè®¡: æ£€æŸ¥äº† {total_combinations_checked} ä¸ªç»„åˆï¼Œå‰ªæäº† {total_combinations_pruned} ä¸ªç»„åˆ")
            if total_combinations_checked + total_combinations_pruned > 0:
                pruning_ratio = total_combinations_pruned / (total_combinations_checked + total_combinations_pruned)
                print(f"  ğŸ“Š å‰ªææ¯”ä¾‹: {pruning_ratio:.2%}")

        return rescued_features

    def analyze_co_occurrence_patterns(self,
                                     problem_groups: ProblemGroups,
                                     subset_selections: List[FeatureSet]) -> Dict[str, Any]:
        """
        Analyze co-occurrence patterns without rescuing features.
        
        Args:
            problem_groups: List of problem groups
            subset_selections: Feature selections from different subsets
            
        Returns:
            Dictionary with co-occurrence analysis results
        """
        co_occurrence_counter = Counter()
        group_analysis = []
        
        for group_idx, group in enumerate(problem_groups):
            group_set = set(group)
            group_co_occurrences = []
            
            for selection in subset_selections:
                intersection = group_set.intersection(selection)
                
                if len(intersection) >= 2:
                    for k in range(2, len(intersection) + 1):
                        for subset in combinations(sorted(list(intersection)), k):
                            co_occurrence_counter[frozenset(subset)] += 1
                            group_co_occurrences.append(subset)
            
            group_analysis.append({
                'group_id': group_idx,
                'group_size': len(group),
                'features': group,
                'co_occurrences_found': len(group_co_occurrences),
                'unique_co_occurrence_patterns': len(set(group_co_occurrences))
            })
        
        # Analyze threshold effects
        threshold_analysis = {}
        for threshold in range(0, 6):
            rescued_count = sum(1 for count in co_occurrence_counter.values() if count > threshold)
            rescued_features = set()
            for feature_set, count in co_occurrence_counter.items():
                if count > threshold:
                    rescued_features.update(feature_set)
            
            threshold_analysis[threshold] = {
                'rescued_patterns': rescued_count,
                'rescued_features': len(rescued_features),
                'rescued_feature_list': list(rescued_features)
            }
        
        return {
            'total_co_occurrence_patterns': len(co_occurrence_counter),
            'group_analysis': group_analysis,
            'threshold_analysis': threshold_analysis,
            'co_occurrence_counter': dict(co_occurrence_counter),
            'most_frequent_patterns': co_occurrence_counter.most_common(10)
        }
    
    def get_rescue_statistics(self, 
                            base_features: FeatureSet,
                            rescued_features: FeatureSet,
                            problem_groups: ProblemGroups) -> Dict[str, Any]:
        """
        Calculate statistics about the rescue process.
        
        Args:
            base_features: Originally selected features
            rescued_features: Features rescued by the process
            problem_groups: Problem groups used in rescue
            
        Returns:
            Dictionary with rescue statistics
        """
        total_features_in_groups = set(feat for group in problem_groups for feat in group)
        
        return {
            'base_feature_count': len(base_features),
            'rescued_feature_count': len(rescued_features),
            'total_final_features': len(base_features | rescued_features),
            'rescue_rate': len(rescued_features) / len(total_features_in_groups) if total_features_in_groups else 0,
            'overlap_with_base': len(base_features & rescued_features),
            'new_features_rescued': len(rescued_features - base_features),
            'problem_groups_count': len(problem_groups),
            'features_in_problem_groups': len(total_features_in_groups)
        }
